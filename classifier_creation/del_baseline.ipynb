{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first neural network with keras tutorial\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('../env_specs/prediction_data.pkl')\n",
    "df.drop(['ProbeFileID','IsTarget'], axis=1, inplace=True)\n",
    "# replace nans with -1\n",
    "df = df.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,Y = df.drop(['label'], axis=1).values, df['label'].values\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "# X_train = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle(arr):\n",
    "    x, y = arr.shape\n",
    "    rows = np.indices((x,y))[0]\n",
    "    cols = [np.random.permutation(y) for _ in range(x)]\n",
    "    return arr[rows, cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples, num_algos = X_train.shape\n",
    "del_orders = np.repeat(np.arange(num_algos).reshape(1, num_algos), repeats=num_samples, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "def auc(y_true, y_pred):\n",
    "    auc = tf.metrics.auc(y_true, y_pred)[1]\n",
    "    K.get_session().run(tf.local_variables_initializer())\n",
    "    return auc\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.25, random_state=42)\n",
    "h1, h2, h3 = 1000, 1000, 500\n",
    "\n",
    "# define the keras model\n",
    "model = Sequential()\n",
    "model.add(Dense(h1, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(h2, input_dim=X.shape[1], activation='relu'))\n",
    "if h3 > 0:\n",
    "    model.add(Dense(h3, input_dim=X.shape[1], activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# compile the keras model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy', auc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def del_algos(X, del_orders):\n",
    "    for i in range(0, X.shape[0], num_algos-1):\n",
    "        for j in range(num_algos-1):\n",
    "            X[i+j, del_orders[i//(num_algos-1),:j+1]] = -1.\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global Epoch: 1\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 637s 1ms/step - loss: 0.3392 - acc: 0.8432 - auc: 0.8677\n",
      "\n",
      "Global Epoch: 2\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 637s 1ms/step - loss: 0.2768 - acc: 0.8751 - auc: 0.9224\n",
      "\n",
      "Global Epoch: 3\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 637s 1ms/step - loss: 0.2636 - acc: 0.8797 - auc: 0.9338\n",
      "\n",
      "Global Epoch: 4\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 644s 1ms/step - loss: 0.2562 - acc: 0.8831 - auc: 0.9397\n",
      "\n",
      "Global Epoch: 5\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 648s 1ms/step - loss: 0.2505 - acc: 0.8858 - auc: 0.9432\n",
      "\n",
      "Global Epoch: 6\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 648s 1ms/step - loss: 0.2512 - acc: 0.8851 - auc: 0.9457\n",
      "\n",
      "Global Epoch: 7\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 571s 913us/step - loss: 0.2493 - acc: 0.8864 - auc: 0.9475\n",
      "\n",
      "Global Epoch: 8\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 572s 915us/step - loss: 0.2478 - acc: 0.8887 - auc: 0.9488\n",
      "\n",
      "Global Epoch: 9\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 573s 917us/step - loss: 0.2516 - acc: 0.8859 - auc: 0.9498\n",
      "\n",
      "Global Epoch: 10\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 571s 914us/step - loss: 0.2479 - acc: 0.8880 - auc: 0.9505\n",
      "\n",
      "Global Epoch: 11\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 572s 915us/step - loss: 0.2489 - acc: 0.8881 - auc: 0.9512\n",
      "\n",
      "Global Epoch: 12\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 696s 1ms/step - loss: 0.2503 - acc: 0.8870 - auc: 0.9517\n",
      "\n",
      "Global Epoch: 13\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 695s 1ms/step - loss: 0.2521 - acc: 0.8870 - auc: 0.9521\n",
      "\n",
      "Global Epoch: 14\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 593s 948us/step - loss: 0.2517 - acc: 0.8871 - auc: 0.9524\n",
      "\n",
      "Global Epoch: 15\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 568s 908us/step - loss: 0.2515 - acc: 0.8877 - auc: 0.9527\n",
      "\n",
      "Global Epoch: 16\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 572s 915us/step - loss: 0.2515 - acc: 0.8877 - auc: 0.9529\n",
      "\n",
      "Global Epoch: 17\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 573s 917us/step - loss: 0.2502 - acc: 0.8882 - auc: 0.9532\n",
      "\n",
      "Global Epoch: 18\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 574s 918us/step - loss: 0.2533 - acc: 0.8858 - auc: 0.9534\n",
      "\n",
      "Global Epoch: 19\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 574s 919us/step - loss: 0.2521 - acc: 0.8863 - auc: 0.9535\n",
      "\n",
      "Global Epoch: 20\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 578s 925us/step - loss: 0.2588 - acc: 0.8840 - auc: 0.9536\n",
      "\n",
      "Global Epoch: 21\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 577s 923us/step - loss: 0.2539 - acc: 0.8864 - auc: 0.9537\n",
      "\n",
      "Global Epoch: 22\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 675s 1ms/step - loss: 0.2559 - acc: 0.8847 - auc: 0.9538\n",
      "\n",
      "Global Epoch: 23\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 676s 1ms/step - loss: 0.2548 - acc: 0.8868 - auc: 0.9539\n",
      "\n",
      "Global Epoch: 24\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 679s 1ms/step - loss: 0.2593 - acc: 0.8860 - auc: 0.9539\n",
      "\n",
      "Global Epoch: 25\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 680s 1ms/step - loss: 0.2580 - acc: 0.8861 - auc: 0.9540\n",
      "\n",
      "Global Epoch: 26\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 727s 1ms/step - loss: 0.2599 - acc: 0.8850 - auc: 0.9540\n",
      "\n",
      "Global Epoch: 27\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 611s 978us/step - loss: 0.2580 - acc: 0.8846 - auc: 0.9541\n",
      "\n",
      "Global Epoch: 28\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 576s 921us/step - loss: 0.2524 - acc: 0.8876 - auc: 0.9541\n",
      "\n",
      "Global Epoch: 29\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 574s 918us/step - loss: 0.2576 - acc: 0.8847 - auc: 0.9542\n",
      "\n",
      "Global Epoch: 30\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 575s 920us/step - loss: 0.2566 - acc: 0.8859 - auc: 0.9542\n",
      "\n",
      "Global Epoch: 31\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 576s 922us/step - loss: 0.2544 - acc: 0.8870 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 32\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 581s 929us/step - loss: 0.2647 - acc: 0.8822 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 33\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 575s 921us/step - loss: 0.2651 - acc: 0.8863 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 34\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 574s 918us/step - loss: 0.2616 - acc: 0.8849 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 35\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 576s 922us/step - loss: 0.2610 - acc: 0.8859 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 36\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 579s 926us/step - loss: 0.2554 - acc: 0.8865 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 37\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 579s 927us/step - loss: 0.2551 - acc: 0.8862 - auc: 0.9544\n",
      "\n",
      "Global Epoch: 38\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 582s 931us/step - loss: 0.2669 - acc: 0.8864 - auc: 0.9544\n",
      "\n",
      "Global Epoch: 39\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 579s 927us/step - loss: 0.2983 - acc: 0.8838 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 40\n",
      "Epoch 1/1\n",
      "625092/625092 [==============================] - 704s 1ms/step - loss: 0.2660 - acc: 0.8838 - auc: 0.9543\n",
      "\n",
      "Global Epoch: 41\n",
      "Epoch 1/1\n",
      "379530/625092 [=================>............] - ETA: 5:57 - loss: 0.2833 - acc: 0.8768 - auc: 0.9543"
     ]
    }
   ],
   "source": [
    "epochs_per_shuffle = 1\n",
    "X_train_rep = np.repeat(X_train, repeats=num_algos-1, axis=0)\n",
    "Y_train_rep = np.repeat(Y_train, repeats=num_algos-1, axis=0)\n",
    "\n",
    "for i in range(50):\n",
    "    del_orders = shuffle(del_orders)\n",
    "    X_train_del = del_algos(np.copy(X_train_rep), del_orders)\n",
    "    \n",
    "    # fit the keras model on the dataset\n",
    "    print(\"Global Epoch: {}\".format(i+1))\n",
    "    model.fit(X_train_del, Y_train_rep, epochs=epochs_per_shuffle, batch_size=30, shuffle=True)\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
